{
    "docs": [
        {
            "location": "/",
            "text": "About the Linked Data Competency Index (LDCI)\n\n\nThe Linked Data Competency Index (LDCI) is a set of topically arranged\nassertions of the knowledge, skills, and habits of mind required for\nprofessional practice in the area of Linked Data.  The LDCI provides an\noverview, or map, of the Linked Data field both for independent learners who\nwant to learn Linked Data methods and technology, and for professors or\ntrainers who want to design and teach courses on the subject.  The assertions\nenumerated in the LDCI are identified with unique, Web-based identifiers (URIs)\nthat can be used to tag corresponding resources, from books or Web-based\ntutorials to YouTube videos, for indexing and retrieval.\n\n\nThe LDCI exemplifies a broader class of documents for describing curriculum\nstandards and learning objectives or outcomes.  There is currently no one\nstandard way to formulate a competency index (or competency framework, as they\nare sometimes called).  The range of subjects to be learned and the\nrequirements for learning are too diverse to be completely normalized.  A\ncompetency index designed to support certification, for example, may need to be\nmore detailed than the LDCI.\n\n\nThe LDCI is a product of two projects: the \nLearning Linked Data\nProject\n, twelve-month planning activity funded in\n2011 and 2012 by the \nInstitute of Museum and Library Services\n(IMLS)\n to explore the creation of a software platform\nfor learning to interpret and create Linked Data, and its follow-up project,\n\nLD4PE\n, or Linked Data for Professional\nEducation, funded by IMLS from 2014 to 2017.  The products of the LD4PE \nProject have been inherited by the Dublin Core Metadata Initiative.\n\n\nTo create this LDCI, the Competency Index Editorial Board invented and tested\nits own stylistic principles.  The Board strove for stylistic coherence,\nconsistent granularity, manageable length, and general readability.  The goal\nwas to formulate a Competency Index that could be printed out by interested\nlearners and profitably be read from start to finish, with language evocative\nenough to convey the general drift of the subject matter even to newcomers.\n\n\nThis document describes those stylistic principles in the form of guidelines\nfor future editors who may want to update or expand the index.  Just as Linked\nData technology is bound to evolve and change over the coming years, this\nCompetency Index can be viewed as a living document.",
            "title": "Overview"
        },
        {
            "location": "/#about-the-linked-data-competency-index-ldci",
            "text": "The Linked Data Competency Index (LDCI) is a set of topically arranged\nassertions of the knowledge, skills, and habits of mind required for\nprofessional practice in the area of Linked Data.  The LDCI provides an\noverview, or map, of the Linked Data field both for independent learners who\nwant to learn Linked Data methods and technology, and for professors or\ntrainers who want to design and teach courses on the subject.  The assertions\nenumerated in the LDCI are identified with unique, Web-based identifiers (URIs)\nthat can be used to tag corresponding resources, from books or Web-based\ntutorials to YouTube videos, for indexing and retrieval.  The LDCI exemplifies a broader class of documents for describing curriculum\nstandards and learning objectives or outcomes.  There is currently no one\nstandard way to formulate a competency index (or competency framework, as they\nare sometimes called).  The range of subjects to be learned and the\nrequirements for learning are too diverse to be completely normalized.  A\ncompetency index designed to support certification, for example, may need to be\nmore detailed than the LDCI.  The LDCI is a product of two projects: the  Learning Linked Data\nProject , twelve-month planning activity funded in\n2011 and 2012 by the  Institute of Museum and Library Services\n(IMLS)  to explore the creation of a software platform\nfor learning to interpret and create Linked Data, and its follow-up project, LD4PE , or Linked Data for Professional\nEducation, funded by IMLS from 2014 to 2017.  The products of the LD4PE \nProject have been inherited by the Dublin Core Metadata Initiative.  To create this LDCI, the Competency Index Editorial Board invented and tested\nits own stylistic principles.  The Board strove for stylistic coherence,\nconsistent granularity, manageable length, and general readability.  The goal\nwas to formulate a Competency Index that could be printed out by interested\nlearners and profitably be read from start to finish, with language evocative\nenough to convey the general drift of the subject matter even to newcomers.  This document describes those stylistic principles in the form of guidelines\nfor future editors who may want to update or expand the index.  Just as Linked\nData technology is bound to evolve and change over the coming years, this\nCompetency Index can be viewed as a living document.",
            "title": "About the Linked Data Competency Index (LDCI)"
        },
        {
            "location": "/D2695955/",
            "text": "LD4PE Competency Index\n\n\nVersion: 2017-06-25 13:39:11 \n\nRaw file: https://github.com... \n\nView at: https://ld4pe.github.com... \n\n\n\n\n[A] Topic Cluster\n\n\n[B] Topic\n\n\n\n\n[C] Competency: Tweet-length assertion of knowledge, skill, or habit of mind.\n\n\n[D] Benchmark: Action demonstrating accomplishment in related competencies.\n\n\n\n\n[A]\n Fundamentals of Resource Description Framework\n\n\n\n\n[B]\n Identity in RDF\n\n\n[C]\n Knows that anything can be named with Uniform Resource Identifiers (URIs), such as agents, places, events, artifacts, and concepts.\n\n\n[C]\n Understands that a \"real-world\" thing may need to be named with a URI distinct from the URI for information about that thing.\n\n\n[C]\n Recognizes that URIs are \"owned\" by the owners of their respective Internet domains.\n\n\n[C]\n Knows that Uniform Resource Identifiers, or URIs (1994), include Uniform Resource Locators (URLs, which locate web pages) as well as location-independent identifiers for physical, conceptual, or web resources.\n\n\n\n\n\n\n[B]\n RDF data model\n\n\n[C]\n Knows the subject-predicate-object component structure of a triple.\n\n\n[C]\n Understands the difference between literals and non-literal resources.\n\n\n[C]\n Understands that URIs and literals denote things in the world (\"resources\") real, imagined, or conceptual.\n\n\n[C]\n Understands that resources are declared to be members (instances) of classes using the property rdf:type.\n\n\n[C]\n Understands the use of datatypes and language tags with literals.\n\n\n[C]\n Understands blank nodes and their uses.\n\n\n[C]\n Understands that QNames define shorthand prefixes for long URIs.\n\n\n[D]\n Uses prefixes for URIs in RDF specifications and data.\n\n\n\n\n\n\n[C]\n Articulates differences between the RDF abstract data model and the XML and relational models.\n\n\n[C]\n Understands the RDF abstract data model as a directed labeled graph.\n\n\n[C]\n Knows graphic conventions for depicting RDF-based models.\n\n\n[D]\n Can use graphing or modeling software to share those models with others.\n\n\n\n\n\n\n[C]\n Understands a named graph as one of the collection of graphs comprising an RDF dataset, with a graph name unique in the context of that dataset.\n\n\n[C]\n Understands how a namespace, informally used in the RDF context for a namespace URI or RDF vocabulary, fundamentally differs from the namespace of data attributes and functions (methods) defined for an object-oriented class.\n\n\n\n\n\n\n[B]\n Related data models\n\n\n[C]\n Grasps essential differences between schemas for syntactic validation (e.g., XML) and for inferencing (RDF Schema).\n\n\n[C]\n Differentiates hierarchical document models (eg, XML) and graph models (RDF).\n\n\n[C]\n Understands how an RDF class (named set of things) fundamentally differs from an object-oriented programming class, which defines a type of object bundling \"state\" (attributes with data values) and \"behavior\" (functions that operate on state).\n\n\n\n\n\n\n[B]\n RDF serialization\n\n\n[C]\n Understands RDF serializations as interchangeable encodings of a given set of triples (RDF graph).\n\n\n[D]\n Uses tools to convert RDF data between different serializations.\n\n\n\n\n\n\n[C]\n Distinguishes the RDF abstract data model and concrete serializations of RDF data.\n\n\n[D]\n Expresses data in serializations such as RDF/XML, N-Triples, Turtle, N3, Trig, JSON-LD, and RDFa.\n\n\n\n\n\n\n\n\n\n\n\n\n[A]\n Fundamentals of Linked Data\n\n\n\n\n[B]\n Web technology\n\n\n[C]\n Knows the origins of the World Wide Web (1989) as a non-linear interactive system, or hypermedia, built on the Internet.\n\n\n[C]\n Understands that Linked Data (2006) extended the notion of a web of documents (the Web) to a notion of a web of finer-grained data (the Linked Data cloud).\n\n\n[C]\n Knows HyperText Markup Language, or HTML (1991+), as a language for \"marking up\" the content and multimedia components of Web pages.\n\n\n[C]\n Knows HTML5 (2014) as a version of HTML extended with support for complex web and mobile applications.\n\n\n[C]\n Knows Hypertext Transfer Protocol, or HTTP (1991+), as the basic technology for resolving hyperlinks and transferring data on the World Wide Web.\n\n\n[C]\n Knows Representational State Transfer, or REST (2000) as a software architectural style whereby browsers can exchange data with web servers, typically on the basis of well-known HTTP actions.\n\n\n\n\n\n\n[B]\n Linked Data principles\n\n\n[C]\n Knows Tim Berners-Lee's principles of Linked Data: use URIs to name things, use HTTP URIs that can be resolved to useful information, and create links to URIs of other things.\n\n\n[C]\n Knows the \"five stars\" of Open Data: put data on the Web, preferably in a structured and preferably non-proprietary format, using URIs to name things, and link to other data.\n\n\n\n\n\n\n[B]\n Linked Data policies and best practices\n\n\n[C]\n Knows the primary organizations related to Linked Data standardization.\n\n\n[D]\n Participates in developing standards and best practice with relevant organizations such as W3C.\n\n\n\n\n\n\n\n\n\n\n[B]\n Non-RDF linked data\n\n\n\n\n[A]\n RDF vocabularies and application profiles\n\n\n\n\n[B]\n Finding RDF-based vocabularies\n        * \n[D]\n [MOVE]Knows portals and registries for finding RDF-based vocabularies.\n        * \n[D]\n Finds properties and classes in the Linked Open Vocabularies (LOV) observatory and explores their versions and dependencies.\n\n\n[B]\n Designing RDF-based vocabularies\n\n\n[C]\n Uses RDF Schema to express semantic relationships within a vocabulary.\n\n\n[D]\n Correctly uses sub-class relationships in support of inference.\n\n\n[D]\n Correctly uses sub-property relationships in support of inference.\n\n\n\n\n\n\n[C]\n Reuses published properties and classes where available.\n\n\n[C]\n Coins namespace URIs, as needed, for any new properties and classes required.\n\n\n[D]\n Drafts a policy for coining URIs for properties and classes.\n\n\n[D]\n Chooses \"hash\"- or \"slash\"-based URI patterns based on requirements.\n\n\n\n\n\n\n[C]\n Knows Web Ontology Language, or OWL (2004), as a RDF vocabulary of properties and classes that extend support for expressive data modeling and automated inferencing (reasoning).\n\n\n[C]\n Knows that the word \"ontology\" is ambiguous, referring to any RDF vocabulary, but more typically a set of OWL classes and properties designed to support inferencing in a specific domain.\n\n\n[C]\n Knows Simple Knowledge Organization System, or SKOS (2009), an RDF vocabulary for expressing concepts that are labeled in natural languages, organized into informal hierarchies, and aggregated into concept schemes.\n\n\n[C]\n Knows SKOS eXtension for Labels, or SKOS-XL (2009), a small set of additional properties for describing and linking lexical labels as instances of the class Label.\n\n\n[C]\n Understands that in a formal sense, a SKOS concept is not an RDF class but an instance and, as such, is not formally associated with a set of instances (\"class extension\").\n\n\n[C]\n Understands that SKOS can express a flexibly associative structure of concepts without enabling the more rigid and automatic inferences typically specified in a class-based OWL ontology.\n\n\n[C]\n Understands that in contrast to OWL sub-class chains, hierarchies of SKOS concepts are designed not to form transitive chains automatically because this is not how humans think or organize information.\n\n\n[C]\n Knows the naming conventions for RDF properties and classes.\n\n\n\n\n\n\n[B]\n Maintaining RDF vocabularies\n\n\n[C]\n Understands policy options for persistence guarantees.\n\n\n[D]\n Can draft a persistence policy.\n\n\n\n\n\n\n\n\n\n\n[B]\n Versioning RDF vocabularies\n\n\n[C]\n Knows technical options for the form, content, and granularity of versions.\n\n\n[C]\n Understands the trade-offs between publishing RDF vocabularies in periodic, numbered releases versus more continual or incremental approaches.\n\n\n[D]\n Can express and justify a versioning policy.\n\n\n\n\n\n\n\n\n\n\n[B]\n Publishing RDF vocabularies\n\n\n[C]\n Understands the typical publication formats for RDF vocabularies and their relative advantages\n\n\n[C]\n Understands the purpose of publishing RDF vocabularies in multiple formats using content negotiation.\n\n\n[C]\n Understands that to be \"dereferencable\", a URI should be usable to retrieve a representation of the resource it identifies.\n\n\n[D]\n Ensures that when dereferenced by a Web browser, a URI returns a representation of the resource in human-readable HTML.\n\n\n[D]\n Ensures that when dereferenced by an RDF application, a URI returns representation of the resource in the requested RDF serialization syntax.\n\n\n\n\n\n\n\n\n\n\n[B]\n Mapping RDF vocabularies\n\n\n[C]\n Understands that the properties of hierarchical subsumption within an RDF vocabulary -- rdfs:subPropertyOf and rdfs:subClassOf -- can also be used to express mappings between vocabularies.\n\n\n[C]\n Understands that owl:equivalentProperty and owl:equivalentClass may be used when equivalencies between properties or between classes are exact.\n\n\n[C]\n Recognizes that owl:sameAs, while popular as a mapping property, has strong formal semantics that can entail unintended inferences.\n\n\n\n\n\n\n[B]\n RDF application profiles\n\n\n[C]\n Identifies real-world entities in an application domain as candidates for RDF classes.\n\n\n[C]\n Identifies resource attributes and relationships between domain entities as candidates for RDF properties.\n\n\n[C]\n Investigates how others have modeled the same or similar application domains.\n\n\n[D]\n Communicates a domain model with words and diagrams.\n\n\n[D]\n Participates in the social process of developing application profiles.\n\n\n\n\n\n\n\n\n\n\n\n\n[A]\n Creating and transforming Linked Data\n\n\n\n\n[B]\n Managing identifiers (URI)\n\n\n[C]\n Understands that to be \"persistent\", a URI must have a stable, well-documented meaning and be plausibly intended to identify a given resource in perpetuity.\n\n\n[C]\n Understands trade-offs between \"opaque\" URIs and URIs using version numbers, server names, dates, application-specific file extensions, query strings or other obsoletable context.\n\n\n[C]\n Recognizes the desirability of a published namespace policy describing an institution's commitment to the persistence and semantic stability of important URIs.\n\n\n\n\n\n\n[B]\n Creating RDF data\n\n\n[C]\n Generates RDF data from non-RDF sources.\n\n\n[C]\n Knows methods for generating RDF data from tabular data in formats such as Comma-Separated Values (CSV).\n\n\n[C]\n Knows methods such as Direct Mapping of Relational Data to RDF (2012) for transforming data from the relational model (keys, values, rows, columns, tables) into RDF graphs.\n\n\n\n\n\n\n[B]\n Versioning RDF data\n\n\n[B]\n RDF data provenance\n\n\n[B]\n Cleaning and reconciling RDF data\n\n\n[C]\n Cleans a dataset by finding and correcting errors, removing duplicates and unwanted data.\n\n\n\n\n\n\n[B]\n Mapping and enriching RDF data\n\n\n[C]\n Uses available resources for named entity recognition, extraction, and reconciliation.\n\n\n\n\n\n\n\n\n[A]\n Interacting with RDF data\n\n\n\n\n[B]\n Finding RDF data\n\n\n[C]\n Knows relevant resources for discovering existing Linked Data datasets.\n\n\n[C]\n Retrieves and accesses RDF data from the \"open Web\".\n\n\n[C]\n Monitors and updates lists which report the status of SPARQL endpoints.\n\n\n[C]\n Uses available vocabularies for dataset description to support their discovery.\n\n\n[C]\n Registers datasets with relevant services for discovery.\n\n\n\n\n\n\n[B]\n Processing RDF data using programming languages.\n\n\n[C]\n Understands how components of the RDF data model (datasets, graphs, statements, and various types of node) are expressed in the RDF library of a given programming language by constructs such as object-oriented classes.\n\n\n[D]\n Uses an RDF programming library to serialize RDF data in available syntaxes.\n\n\n[D]\n Uses RDF-specific programming methods to iterate over components of RDF data.\n\n\n[D]\n Uses RDF-library-specific convenience representations for common RDF vocabularies such as RDF, Dublin Core, and SKOS.\n\n\n\n\n\n\n[C]\n Programatically associates namespaces to prefixes for use in serializing RDF or when parsing SPARQL queries.\n\n\n[D]\n Uses RDF programming libraries to extract RDF data from CSV files, databases, or web pages.\n\n\n[D]\n Uses RDF programming libraries to persistently stores triples in memory, on disk, or to interact with triple stores.\n\n\n[D]\n Programatically infers triples using custom functions or methods.\n\n\n\n\n\n\n[C]\n Understands how the pattern matching of SPARQL queries can be expressed using functionally equivalent constructs in RDF programming libraries.\n\n\n[D]\n Uses RDF-specific programming methods to query RDF data and save the results for further processing.\n\n\n[D]\n Uses utilities and convenience functions the provide shortcuts for frequently used patterns, such as matching the multiple label properties used in real data.\n\n\n[D]\n Uses RDF libraries to process various types of SPARQL query result.\n\n\n\n\n\n\n\n\n\n\n[B]\n Querying RDF data\n\n\n[C]\n Understands that a SPARQL query matches an RDF graph against a pattern of triples with fixed and variable values.\n\n\n[C]\n Understands the basic syntax of a SPARQL query.\n\n\n[D]\n Uses angle brackets for delimiting URIs.\n\n\n[D]\n Uses question marks for indicating variables.\n\n\n[D]\n Uses PREFIX for base URIs.\n\n\n\n\n\n\n[C]\n Demonstrates a working knowledge of the forms and uses of SPARQL result sets (SELECT, CONSTRUCT, DESCRIBE, and ASK).\n\n\n[D]\n Uses the SELECT clause to identify the variables to appear in a table of query results.\n\n\n[D]\n Uses the WHERE clause to provide the graph pattern to match against the graph data.\n\n\n[D]\n Uses variables in SELECT and WHERE clauses to yield a table of results.\n\n\n[D]\n Uses ASK for a True/False result test for a match to a query pattern.\n\n\n[D]\n Uses DESCRIBE to extract a single graph containing RDF data about resources.\n\n\n[D]\n Uses CONSTRUCT to extract and transform results into a single RDF graph specified by a graph template.\n\n\n[D]\n Uses FROM to formulate queries with URLs and local files.\n\n\n\n\n\n\n[C]\n Understands how to combine and filter graph patterns using operators such as UNION, OPTIONAL, FILTER, and MINUS.\n\n\n[D]\n Uses UNION to formulate queries with multiple possible graph patterns.\n\n\n[D]\n Uses OPTIONAL to formulate queries to return the values of optional variables when available.\n\n\n[D]\n Uses FILTER to formulates queries that eliminate solutions from a result set.\n\n\n[D]\n Uses NOT EXISTS to limit whether a given graph pattern exists in the data.\n\n\n[D]\n Uses MINUS to remove matches from a result based on the evaluation of two patterns.\n\n\n[D]\n Uses NOT IN to restrict a variable to not being in a given set of values.\n\n\n\n\n\n\n[C]\n Understands the major SPARQL result set modifiers, e.g., to limit or sort results, or to return distinct results only once.\n\n\n[D]\n Uses ORDER BY to define ordering conditions by variable, function call, or expression.\n\n\n[D]\n Uses DISTINCT to ensure solutions in the sequence are unique.\n\n\n[D]\n Uses OFFSET to control where the solutions processed start in the overall sequence of solutions.\n\n\n[D]\n Uses LIMIT to restrict the number of solutions processed for query results.\n\n\n[D]\n Uses projection to transform a solution sequence into one involving only a subset of the variables.\n\n\n\n\n\n\n[C]\n Understands the use of SPARQL functions and operators.\n\n\n[D]\n Uses the regular expression (regex()) function for string matching.\n\n\n[D]\n Uses aggregates to apply expressions over groups of solutions (GROUP BY, COUNT, SUM, AVG, MIN) for partitioning results, evaluating projections, and filtering.\n\n\n[D]\n Uses the lang() function to return the language tag of an RDF literal.\n\n\n[D]\n Uses the langMatches() function to match a language tag against a language range.\n\n\n[D]\n Uses the xsd:decimal(expn) function to convert an expression to an integer.\n\n\n[D]\n Uses the GROUP BY clause to transforms a result set so that only one row will appear for each unique set of grouping variables.\n\n\n[D]\n Uses the HAVING clause to apply a filter to the result set after grouping.\n\n\n\n\n\n\n[C]\n Differentiates between a Default Graph and a Named Graph, and formulates queries using the GRAPH clause.\n\n\n[D]\n Formulates advanced queries using FROM NAMED and GRAPH on local data.\n\n\n[D]\n Formulates advanced queries using FROM NAMED on remote data.\n\n\n[D]\n Formulates advanced queries on data containing blank nodes.\n\n\n[D]\n Formulates advanced queries using subqueries.\n\n\n\n\n\n\n[C]\n Uses a temporary variable to extend a query.\n\n\n[C]\n Understands the role of Property Paths and how they are formed by combining predicates with regular expression-like operators.\n\n\n[C]\n Understands the concept of Federated Searches.\n\n\n[D]\n Formulates advanced queries on a remote SPARQL endpoint using the SERVICE directive.\n\n\n[D]\n Uses federated query to query over a local graph store and one or more other SPARQL endpoints.\n\n\n[D]\n Pulls data from a different SPARQL endpoints in one single query using the SERVICE directive.\n\n\n\n\n\n\n[C]\n Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by a third party tools and APIs.\n\n\n[C]\n Reads and understands high-level descriptions of the classes and properties of a dataset in order to write queries.\n\n\n[C]\n Uses available tools, servers, and endpoints to issue queries against a dataset.\n\n\n[D]\n Execute SPARQL queries using the Jena ARQ command-line utility.\n\n\n[D]\n Queries multiple local data files using ARQ.\n\n\n[D]\n Uses ARQ to evaluate queries on local data.\n\n\n[D]\n Uses Fuseki server to evaluate queries on a dataset.\n\n\n[D]\n Queries multiple data files using Fuseki.\n\n\n[D]\n Accesses DBPedia's SNORQL/SPARQL endpoint and issues simple queries.\n\n\n\n\n\n\n\n\n\n\n[B]\n Visualizing RDF data\n\n\n[C]\n Uses publicly available tools to visualize data.\n\n\n[D]\n Uses Google FusionTables to create maps and charts.\n\n\n\n\n\n\n[C]\n Distills results taken from large datasets so that visualizations are human-friendly.\n\n\n[C]\n Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by third party tools and APIs.\n\n\n\n\n\n\n[B]\n Reasoning over RDF data\n\n\n[C]\n Understands the principles and practice of inferencing.\n\n\n[C]\n Uses common entailment regimes and understands their uses.\n\n\n[C]\n Understands the role of formally declared domains and ranges for inferencing.\n\n\n[C]\n Understands how reasoning can be used for integrating diverse datasets.\n\n\n[C]\n Knows that Web Ontology Language (OWL) is available in multiple \"flavors\" that are variously optimized for expressivity, performant reasoning, or for applications involving databases or business rules.\n\n\n[C]\n Understands that OWL Full supports all available constructs and is most appropriately used when reasoning performance is not a concern.\n\n\n\n\n\n\n[B]\n Assessing RDF data quality\n\n\n[B]\n RDF data analytics\n\n\n[C]\n Uses available ontology browsing tools to explore the ontologies used in a particular dataset.\n\n\n\n\n\n\n[B]\n Manipulating RDF data\n\n\n[C]\n Knows the SPARQL 1.1 Update language for updating, creating, and removing RDF graphs in a Graph Store\n\n\n[D]\n Uses INSERT/DELETE to update triples.\n\n\n[D]\n Uses a CONSTRUCT query to preview changes before executing an INSERT/DELETE operation.\n\n\n\n\n\n\n[C]\n Knows the SPARQL 1.1 Graph Store HTTP protocol for updating graphs on a web server (in \"restful\" style).\n\n\n[D]\n Uses GET to retrieve triples from a default graph or a named graph.\n\n\n[D]\n Uses PUT to insert set of triples into a new graph (or replace an existing graph).\n\n\n[D]\n Uses DELETE to remove a graph.\n\n\n[D]\n Uses POST to add triples to an existing graph.\n\n\n[D]\n Uses proper syntax to request specific media types, such as Turtle.\n\n\n\n\n\n\n[C]\n Understands the difference between SQL query language (which operates on database tables) and SPARQL (which operates on RDF graphs).\n\n\n\n\n\n\n\n\n[A]\n Creating Linked Data applications\n\n\n\n\n[B]\n Storing RDF data",
            "title": "Competency Index"
        },
        {
            "location": "/D2695955/#ld4pe-competency-index",
            "text": "Version: 2017-06-25 13:39:11  \nRaw file: https://github.com...  \nView at: https://ld4pe.github.com...",
            "title": "LD4PE Competency Index"
        },
        {
            "location": "/D2695955/#a-topic-cluster",
            "text": "",
            "title": "[A] Topic Cluster"
        },
        {
            "location": "/D2695955/#b-topic",
            "text": "[C] Competency: Tweet-length assertion of knowledge, skill, or habit of mind.  [D] Benchmark: Action demonstrating accomplishment in related competencies.",
            "title": "[B] Topic"
        },
        {
            "location": "/D2695955/#a-fundamentals-of-resource-description-framework",
            "text": "[B]  Identity in RDF  [C]  Knows that anything can be named with Uniform Resource Identifiers (URIs), such as agents, places, events, artifacts, and concepts.  [C]  Understands that a \"real-world\" thing may need to be named with a URI distinct from the URI for information about that thing.  [C]  Recognizes that URIs are \"owned\" by the owners of their respective Internet domains.  [C]  Knows that Uniform Resource Identifiers, or URIs (1994), include Uniform Resource Locators (URLs, which locate web pages) as well as location-independent identifiers for physical, conceptual, or web resources.    [B]  RDF data model  [C]  Knows the subject-predicate-object component structure of a triple.  [C]  Understands the difference between literals and non-literal resources.  [C]  Understands that URIs and literals denote things in the world (\"resources\") real, imagined, or conceptual.  [C]  Understands that resources are declared to be members (instances) of classes using the property rdf:type.  [C]  Understands the use of datatypes and language tags with literals.  [C]  Understands blank nodes and their uses.  [C]  Understands that QNames define shorthand prefixes for long URIs.  [D]  Uses prefixes for URIs in RDF specifications and data.    [C]  Articulates differences between the RDF abstract data model and the XML and relational models.  [C]  Understands the RDF abstract data model as a directed labeled graph.  [C]  Knows graphic conventions for depicting RDF-based models.  [D]  Can use graphing or modeling software to share those models with others.    [C]  Understands a named graph as one of the collection of graphs comprising an RDF dataset, with a graph name unique in the context of that dataset.  [C]  Understands how a namespace, informally used in the RDF context for a namespace URI or RDF vocabulary, fundamentally differs from the namespace of data attributes and functions (methods) defined for an object-oriented class.    [B]  Related data models  [C]  Grasps essential differences between schemas for syntactic validation (e.g., XML) and for inferencing (RDF Schema).  [C]  Differentiates hierarchical document models (eg, XML) and graph models (RDF).  [C]  Understands how an RDF class (named set of things) fundamentally differs from an object-oriented programming class, which defines a type of object bundling \"state\" (attributes with data values) and \"behavior\" (functions that operate on state).    [B]  RDF serialization  [C]  Understands RDF serializations as interchangeable encodings of a given set of triples (RDF graph).  [D]  Uses tools to convert RDF data between different serializations.    [C]  Distinguishes the RDF abstract data model and concrete serializations of RDF data.  [D]  Expresses data in serializations such as RDF/XML, N-Triples, Turtle, N3, Trig, JSON-LD, and RDFa.",
            "title": "[A] Fundamentals of Resource Description Framework"
        },
        {
            "location": "/D2695955/#a-fundamentals-of-linked-data",
            "text": "[B]  Web technology  [C]  Knows the origins of the World Wide Web (1989) as a non-linear interactive system, or hypermedia, built on the Internet.  [C]  Understands that Linked Data (2006) extended the notion of a web of documents (the Web) to a notion of a web of finer-grained data (the Linked Data cloud).  [C]  Knows HyperText Markup Language, or HTML (1991+), as a language for \"marking up\" the content and multimedia components of Web pages.  [C]  Knows HTML5 (2014) as a version of HTML extended with support for complex web and mobile applications.  [C]  Knows Hypertext Transfer Protocol, or HTTP (1991+), as the basic technology for resolving hyperlinks and transferring data on the World Wide Web.  [C]  Knows Representational State Transfer, or REST (2000) as a software architectural style whereby browsers can exchange data with web servers, typically on the basis of well-known HTTP actions.    [B]  Linked Data principles  [C]  Knows Tim Berners-Lee's principles of Linked Data: use URIs to name things, use HTTP URIs that can be resolved to useful information, and create links to URIs of other things.  [C]  Knows the \"five stars\" of Open Data: put data on the Web, preferably in a structured and preferably non-proprietary format, using URIs to name things, and link to other data.    [B]  Linked Data policies and best practices  [C]  Knows the primary organizations related to Linked Data standardization.  [D]  Participates in developing standards and best practice with relevant organizations such as W3C.      [B]  Non-RDF linked data",
            "title": "[A] Fundamentals of Linked Data"
        },
        {
            "location": "/D2695955/#a-rdf-vocabularies-and-application-profiles",
            "text": "[B]  Finding RDF-based vocabularies\n        *  [D]  [MOVE]Knows portals and registries for finding RDF-based vocabularies.\n        *  [D]  Finds properties and classes in the Linked Open Vocabularies (LOV) observatory and explores their versions and dependencies.  [B]  Designing RDF-based vocabularies  [C]  Uses RDF Schema to express semantic relationships within a vocabulary.  [D]  Correctly uses sub-class relationships in support of inference.  [D]  Correctly uses sub-property relationships in support of inference.    [C]  Reuses published properties and classes where available.  [C]  Coins namespace URIs, as needed, for any new properties and classes required.  [D]  Drafts a policy for coining URIs for properties and classes.  [D]  Chooses \"hash\"- or \"slash\"-based URI patterns based on requirements.    [C]  Knows Web Ontology Language, or OWL (2004), as a RDF vocabulary of properties and classes that extend support for expressive data modeling and automated inferencing (reasoning).  [C]  Knows that the word \"ontology\" is ambiguous, referring to any RDF vocabulary, but more typically a set of OWL classes and properties designed to support inferencing in a specific domain.  [C]  Knows Simple Knowledge Organization System, or SKOS (2009), an RDF vocabulary for expressing concepts that are labeled in natural languages, organized into informal hierarchies, and aggregated into concept schemes.  [C]  Knows SKOS eXtension for Labels, or SKOS-XL (2009), a small set of additional properties for describing and linking lexical labels as instances of the class Label.  [C]  Understands that in a formal sense, a SKOS concept is not an RDF class but an instance and, as such, is not formally associated with a set of instances (\"class extension\").  [C]  Understands that SKOS can express a flexibly associative structure of concepts without enabling the more rigid and automatic inferences typically specified in a class-based OWL ontology.  [C]  Understands that in contrast to OWL sub-class chains, hierarchies of SKOS concepts are designed not to form transitive chains automatically because this is not how humans think or organize information.  [C]  Knows the naming conventions for RDF properties and classes.    [B]  Maintaining RDF vocabularies  [C]  Understands policy options for persistence guarantees.  [D]  Can draft a persistence policy.      [B]  Versioning RDF vocabularies  [C]  Knows technical options for the form, content, and granularity of versions.  [C]  Understands the trade-offs between publishing RDF vocabularies in periodic, numbered releases versus more continual or incremental approaches.  [D]  Can express and justify a versioning policy.      [B]  Publishing RDF vocabularies  [C]  Understands the typical publication formats for RDF vocabularies and their relative advantages  [C]  Understands the purpose of publishing RDF vocabularies in multiple formats using content negotiation.  [C]  Understands that to be \"dereferencable\", a URI should be usable to retrieve a representation of the resource it identifies.  [D]  Ensures that when dereferenced by a Web browser, a URI returns a representation of the resource in human-readable HTML.  [D]  Ensures that when dereferenced by an RDF application, a URI returns representation of the resource in the requested RDF serialization syntax.      [B]  Mapping RDF vocabularies  [C]  Understands that the properties of hierarchical subsumption within an RDF vocabulary -- rdfs:subPropertyOf and rdfs:subClassOf -- can also be used to express mappings between vocabularies.  [C]  Understands that owl:equivalentProperty and owl:equivalentClass may be used when equivalencies between properties or between classes are exact.  [C]  Recognizes that owl:sameAs, while popular as a mapping property, has strong formal semantics that can entail unintended inferences.    [B]  RDF application profiles  [C]  Identifies real-world entities in an application domain as candidates for RDF classes.  [C]  Identifies resource attributes and relationships between domain entities as candidates for RDF properties.  [C]  Investigates how others have modeled the same or similar application domains.  [D]  Communicates a domain model with words and diagrams.  [D]  Participates in the social process of developing application profiles.",
            "title": "[A] RDF vocabularies and application profiles"
        },
        {
            "location": "/D2695955/#a-creating-and-transforming-linked-data",
            "text": "[B]  Managing identifiers (URI)  [C]  Understands that to be \"persistent\", a URI must have a stable, well-documented meaning and be plausibly intended to identify a given resource in perpetuity.  [C]  Understands trade-offs between \"opaque\" URIs and URIs using version numbers, server names, dates, application-specific file extensions, query strings or other obsoletable context.  [C]  Recognizes the desirability of a published namespace policy describing an institution's commitment to the persistence and semantic stability of important URIs.    [B]  Creating RDF data  [C]  Generates RDF data from non-RDF sources.  [C]  Knows methods for generating RDF data from tabular data in formats such as Comma-Separated Values (CSV).  [C]  Knows methods such as Direct Mapping of Relational Data to RDF (2012) for transforming data from the relational model (keys, values, rows, columns, tables) into RDF graphs.    [B]  Versioning RDF data  [B]  RDF data provenance  [B]  Cleaning and reconciling RDF data  [C]  Cleans a dataset by finding and correcting errors, removing duplicates and unwanted data.    [B]  Mapping and enriching RDF data  [C]  Uses available resources for named entity recognition, extraction, and reconciliation.",
            "title": "[A] Creating and transforming Linked Data"
        },
        {
            "location": "/D2695955/#a-interacting-with-rdf-data",
            "text": "[B]  Finding RDF data  [C]  Knows relevant resources for discovering existing Linked Data datasets.  [C]  Retrieves and accesses RDF data from the \"open Web\".  [C]  Monitors and updates lists which report the status of SPARQL endpoints.  [C]  Uses available vocabularies for dataset description to support their discovery.  [C]  Registers datasets with relevant services for discovery.    [B]  Processing RDF data using programming languages.  [C]  Understands how components of the RDF data model (datasets, graphs, statements, and various types of node) are expressed in the RDF library of a given programming language by constructs such as object-oriented classes.  [D]  Uses an RDF programming library to serialize RDF data in available syntaxes.  [D]  Uses RDF-specific programming methods to iterate over components of RDF data.  [D]  Uses RDF-library-specific convenience representations for common RDF vocabularies such as RDF, Dublin Core, and SKOS.    [C]  Programatically associates namespaces to prefixes for use in serializing RDF or when parsing SPARQL queries.  [D]  Uses RDF programming libraries to extract RDF data from CSV files, databases, or web pages.  [D]  Uses RDF programming libraries to persistently stores triples in memory, on disk, or to interact with triple stores.  [D]  Programatically infers triples using custom functions or methods.    [C]  Understands how the pattern matching of SPARQL queries can be expressed using functionally equivalent constructs in RDF programming libraries.  [D]  Uses RDF-specific programming methods to query RDF data and save the results for further processing.  [D]  Uses utilities and convenience functions the provide shortcuts for frequently used patterns, such as matching the multiple label properties used in real data.  [D]  Uses RDF libraries to process various types of SPARQL query result.      [B]  Querying RDF data  [C]  Understands that a SPARQL query matches an RDF graph against a pattern of triples with fixed and variable values.  [C]  Understands the basic syntax of a SPARQL query.  [D]  Uses angle brackets for delimiting URIs.  [D]  Uses question marks for indicating variables.  [D]  Uses PREFIX for base URIs.    [C]  Demonstrates a working knowledge of the forms and uses of SPARQL result sets (SELECT, CONSTRUCT, DESCRIBE, and ASK).  [D]  Uses the SELECT clause to identify the variables to appear in a table of query results.  [D]  Uses the WHERE clause to provide the graph pattern to match against the graph data.  [D]  Uses variables in SELECT and WHERE clauses to yield a table of results.  [D]  Uses ASK for a True/False result test for a match to a query pattern.  [D]  Uses DESCRIBE to extract a single graph containing RDF data about resources.  [D]  Uses CONSTRUCT to extract and transform results into a single RDF graph specified by a graph template.  [D]  Uses FROM to formulate queries with URLs and local files.    [C]  Understands how to combine and filter graph patterns using operators such as UNION, OPTIONAL, FILTER, and MINUS.  [D]  Uses UNION to formulate queries with multiple possible graph patterns.  [D]  Uses OPTIONAL to formulate queries to return the values of optional variables when available.  [D]  Uses FILTER to formulates queries that eliminate solutions from a result set.  [D]  Uses NOT EXISTS to limit whether a given graph pattern exists in the data.  [D]  Uses MINUS to remove matches from a result based on the evaluation of two patterns.  [D]  Uses NOT IN to restrict a variable to not being in a given set of values.    [C]  Understands the major SPARQL result set modifiers, e.g., to limit or sort results, or to return distinct results only once.  [D]  Uses ORDER BY to define ordering conditions by variable, function call, or expression.  [D]  Uses DISTINCT to ensure solutions in the sequence are unique.  [D]  Uses OFFSET to control where the solutions processed start in the overall sequence of solutions.  [D]  Uses LIMIT to restrict the number of solutions processed for query results.  [D]  Uses projection to transform a solution sequence into one involving only a subset of the variables.    [C]  Understands the use of SPARQL functions and operators.  [D]  Uses the regular expression (regex()) function for string matching.  [D]  Uses aggregates to apply expressions over groups of solutions (GROUP BY, COUNT, SUM, AVG, MIN) for partitioning results, evaluating projections, and filtering.  [D]  Uses the lang() function to return the language tag of an RDF literal.  [D]  Uses the langMatches() function to match a language tag against a language range.  [D]  Uses the xsd:decimal(expn) function to convert an expression to an integer.  [D]  Uses the GROUP BY clause to transforms a result set so that only one row will appear for each unique set of grouping variables.  [D]  Uses the HAVING clause to apply a filter to the result set after grouping.    [C]  Differentiates between a Default Graph and a Named Graph, and formulates queries using the GRAPH clause.  [D]  Formulates advanced queries using FROM NAMED and GRAPH on local data.  [D]  Formulates advanced queries using FROM NAMED on remote data.  [D]  Formulates advanced queries on data containing blank nodes.  [D]  Formulates advanced queries using subqueries.    [C]  Uses a temporary variable to extend a query.  [C]  Understands the role of Property Paths and how they are formed by combining predicates with regular expression-like operators.  [C]  Understands the concept of Federated Searches.  [D]  Formulates advanced queries on a remote SPARQL endpoint using the SERVICE directive.  [D]  Uses federated query to query over a local graph store and one or more other SPARQL endpoints.  [D]  Pulls data from a different SPARQL endpoints in one single query using the SERVICE directive.    [C]  Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by a third party tools and APIs.  [C]  Reads and understands high-level descriptions of the classes and properties of a dataset in order to write queries.  [C]  Uses available tools, servers, and endpoints to issue queries against a dataset.  [D]  Execute SPARQL queries using the Jena ARQ command-line utility.  [D]  Queries multiple local data files using ARQ.  [D]  Uses ARQ to evaluate queries on local data.  [D]  Uses Fuseki server to evaluate queries on a dataset.  [D]  Queries multiple data files using Fuseki.  [D]  Accesses DBPedia's SNORQL/SPARQL endpoint and issues simple queries.      [B]  Visualizing RDF data  [C]  Uses publicly available tools to visualize data.  [D]  Uses Google FusionTables to create maps and charts.    [C]  Distills results taken from large datasets so that visualizations are human-friendly.  [C]  Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by third party tools and APIs.    [B]  Reasoning over RDF data  [C]  Understands the principles and practice of inferencing.  [C]  Uses common entailment regimes and understands their uses.  [C]  Understands the role of formally declared domains and ranges for inferencing.  [C]  Understands how reasoning can be used for integrating diverse datasets.  [C]  Knows that Web Ontology Language (OWL) is available in multiple \"flavors\" that are variously optimized for expressivity, performant reasoning, or for applications involving databases or business rules.  [C]  Understands that OWL Full supports all available constructs and is most appropriately used when reasoning performance is not a concern.    [B]  Assessing RDF data quality  [B]  RDF data analytics  [C]  Uses available ontology browsing tools to explore the ontologies used in a particular dataset.    [B]  Manipulating RDF data  [C]  Knows the SPARQL 1.1 Update language for updating, creating, and removing RDF graphs in a Graph Store  [D]  Uses INSERT/DELETE to update triples.  [D]  Uses a CONSTRUCT query to preview changes before executing an INSERT/DELETE operation.    [C]  Knows the SPARQL 1.1 Graph Store HTTP protocol for updating graphs on a web server (in \"restful\" style).  [D]  Uses GET to retrieve triples from a default graph or a named graph.  [D]  Uses PUT to insert set of triples into a new graph (or replace an existing graph).  [D]  Uses DELETE to remove a graph.  [D]  Uses POST to add triples to an existing graph.  [D]  Uses proper syntax to request specific media types, such as Turtle.    [C]  Understands the difference between SQL query language (which operates on database tables) and SPARQL (which operates on RDF graphs).",
            "title": "[A] Interacting with RDF data"
        },
        {
            "location": "/D2695955/#a-creating-linked-data-applications",
            "text": "[B]  Storing RDF data",
            "title": "[A] Creating Linked Data applications"
        },
        {
            "location": "/structure/",
            "text": "\"Competency Index for Linked Data\": style and editorial guidelines\n\n\n2016-12-24: @TODO: constitution, members, process (members, meetings, decision \nmethodology).  Stylistic consistency.\n\n\n\n\nDescribe Editorial Board process: constitution, members, process (members, meetings, decision method)\n\n\nPick one pull request, link to its diff\n\n\nWrite to CIEB: status of pull requests. Proposal Dec telecon. Still using \"manual\" update. Propose style cleanup.\n\n\n\n\nTom Baker, November 2016\n\n\nThe Competency Index for Linked Data (CI) is a set of topically arranged\nassertions of the knowledge, skills, and habits of mind required for\nprofessional practice in the area of Linked Data.  The target audience for the\nCI consists of independent learners who want to learn Linked Data methods and\ntechnology, and professors or trainers who want to design and teach courses.\n\n\nThe CI exemplifies a broader class of documents, or frameworks, for describing\ncurriculum standards and learning objectives or outcomes.  There is no one\nstandard way to formulate a competency framework; the range of subjects to be\nlearned and the requirements for learning are too diverse to be completely\nnormalized.  Competency indexes designed to support certification, for example,\nmay need to be more detailed than this index.\n\n\nTo create this index, the Competency Index Editorial Board invented and tested\nits own stylistic principles.  The Board strove for stylistic coherence,\nconsistent granularity, manageable length, and general readability.  The goal\nwas to formulate a Competency Index that could be printed out by interested\nlearners and profitably be read from start to finish, with language evocative\nenough to convey the general drift of the subject matter even to newcomers.\n\n\nThis document describes those stylistic principles in the form of guidelines\nfor future editors who may want to update or expand the index.  Just as Linked\nData technology is bound to evolve and change over the coming years, this\nCompetency Index can be viewed as a living document.\n\n\nStructure of the Competency Index\n\n\nThe Competency Index is structured into \ntopics\n (and higher-level \ntopic\nclusters\n), \ncompetencies\n, and \nbenchmarks\n:\n\n\nTopics\n.  A topic is a theme under which a set of competencies are grouped,\nsuch as \nDesigning RDF-based vocabularies\n and \nMaintaining RDF vocabularies\n.\nTopics are grouped under higher-level topic clusters, such as \nRDF vocabularies\nand application profiles\n.  \n\n\nCompetencies\n.  A competency is a brief phrase characterizing knowledge\n(facts, insights, habits of mind, or skills) that may be learned.\nCompetencies may be used as building blocks for constructing self-learning\nplans, university courses, or even entire curricula.  Under the topic \nQuerying\nRDF Data\n, for example, one finds two competencies:\n\n\n\n\nUnderstands that a SPARQL query matches an RDF graph against a pattern of\n  triples with fixed and variable values.\n\n\nUnderstands the basic syntax of a SPARQL query.\n\n\n\n\nBenchmarks\n.  A benchmark is a brief phrase describing an action that can\ndemonstrate accomplishment in a given competency.  If competencies are about\nlearning, benchmarks are about doing.  Benchmarks may be used for devising\nhomework assignments, exam questions, or self-testing checklists.  Under the\ncompetency \nUnderstands the basic syntax of a SPARQL query\n, for example, one\nfinds three benchmarks:\n\n\n\n\nUses angle brackets for delimiting URIs.\n\n\nUses question marks for indicating variables.\n\n\nUses PREFIX for base URIs.\n\n\n\n\nThe strictly topical structure of the Competency Index entailed the following \ndesign choices:\n\n\n\n\nThe CI avoids classifying competencies or benchmarks by level of difficulty\n  because it makes no particular assumptions about the background or skills\n  sets of learners.  Concepts that are comparatively easy for a library science \n  student may be hard for a computer science student, and vice versa.\n\n\nThe CI avoids implying an inherent order to the topics.\n\n\n\n\nWriting effective competencies and benchmarks\n\n\n\n\n\n\nLimit each competency or benchmark to one sentence of circa 140 characters,\n   the length of a Twitter posting.  If a thought seems to require more words,\n   consider splitting it into two simpler thoughts.\n\n\n\n\n\n\nBegin each competency or benchmark with an action verb.  Competencies, which\n   are about learning and understanding, start with verbs such as \nunderstands\n,\n   \nknows\n, \nrecognizes\n, or \ndifferentiates\n.  Benchmarks, which are about\n   doing, start with verbs such as \nuses\n, \nexpresses\n, or \ndemonstrates, \n   or with more task-specific verbs such as _distills\n or\n   \nconverts\n.\n\n\n\n\n\n\nSpell out acronyms at least once.  For example, the competency that\n   introducing OWL says: \nKnows Web Ontology Language, or OWL (2004), an RDF\n   vocabulary of properties and classes that extend support for expressive data\n   modeling and automated inferencing (reasoning).\n  Use your judgement in this\n   regard; in a competency index about Linked Data, frequently used acronyms\n   such as \nOWL\n and \nURI\n need only be spelled out once.\n\n\n\n\n\n\nInclude historical context if possible.  Linked Data technology has largely\n   evolved over the past quarter century and continues to evolve.  Knowing the\n   year when a technology or concept was introduced helps readers, for example:\n   \nWorld Wide Web (1989)\n, \nHTTP (1991+)\n, \nURIs (1994)\n, \nOWL (2004)\n, and \n   \nLinked Data (2006)\n.\n\n\n\n\n\n\nInclude enough detail to characterize the nature of competency in a domain.\n   Competencies and benchmarks should not aim at covering all features of a\n   technology, in the manner of a reference manual.  Attempts at\n   comprehensiveness risk making the CI brittle in the face of inevitable\n   change, and they risk making the CI boring to read.\n\n\n\n\n\n\nDraw attention to ambiguity in the definition or use of terminology.  One \n   competency reads: \nKnows that the word \"ontology\" is amiguous, referring to \n   any RDF vocabulary, but more typically a set of OWL classes and properties \n   designed to support inferencing in a specific domain\n.\n\n\n\n\n\n\nEnlarge the set of topics covered by the CI with prudence, bearing in mind\n   adding topic clusters enlarges the scope of the CI as a whole.  The starter\n   set of topics for this CI were originally brainstormed in 2012, at a\n   workshop of experts convened by a previous IMLS-funded project, Learning\n   Linked Data, and there is no expectation that this scope will remain static.\n   For example, there is a recognized need to define competencies related to\n   knowledge organization systems, but this feels like a topic big enough to \n   require its own competency index.",
            "title": "Structure"
        },
        {
            "location": "/structure/#competency-index-for-linked-data-style-and-editorial-guidelines",
            "text": "2016-12-24: @TODO: constitution, members, process (members, meetings, decision \nmethodology).  Stylistic consistency.   Describe Editorial Board process: constitution, members, process (members, meetings, decision method)  Pick one pull request, link to its diff  Write to CIEB: status of pull requests. Proposal Dec telecon. Still using \"manual\" update. Propose style cleanup.   Tom Baker, November 2016  The Competency Index for Linked Data (CI) is a set of topically arranged\nassertions of the knowledge, skills, and habits of mind required for\nprofessional practice in the area of Linked Data.  The target audience for the\nCI consists of independent learners who want to learn Linked Data methods and\ntechnology, and professors or trainers who want to design and teach courses.  The CI exemplifies a broader class of documents, or frameworks, for describing\ncurriculum standards and learning objectives or outcomes.  There is no one\nstandard way to formulate a competency framework; the range of subjects to be\nlearned and the requirements for learning are too diverse to be completely\nnormalized.  Competency indexes designed to support certification, for example,\nmay need to be more detailed than this index.  To create this index, the Competency Index Editorial Board invented and tested\nits own stylistic principles.  The Board strove for stylistic coherence,\nconsistent granularity, manageable length, and general readability.  The goal\nwas to formulate a Competency Index that could be printed out by interested\nlearners and profitably be read from start to finish, with language evocative\nenough to convey the general drift of the subject matter even to newcomers.  This document describes those stylistic principles in the form of guidelines\nfor future editors who may want to update or expand the index.  Just as Linked\nData technology is bound to evolve and change over the coming years, this\nCompetency Index can be viewed as a living document.",
            "title": "\"Competency Index for Linked Data\": style and editorial guidelines"
        },
        {
            "location": "/structure/#structure-of-the-competency-index",
            "text": "The Competency Index is structured into  topics  (and higher-level  topic\nclusters ),  competencies , and  benchmarks :  Topics .  A topic is a theme under which a set of competencies are grouped,\nsuch as  Designing RDF-based vocabularies  and  Maintaining RDF vocabularies .\nTopics are grouped under higher-level topic clusters, such as  RDF vocabularies\nand application profiles .    Competencies .  A competency is a brief phrase characterizing knowledge\n(facts, insights, habits of mind, or skills) that may be learned.\nCompetencies may be used as building blocks for constructing self-learning\nplans, university courses, or even entire curricula.  Under the topic  Querying\nRDF Data , for example, one finds two competencies:   Understands that a SPARQL query matches an RDF graph against a pattern of\n  triples with fixed and variable values.  Understands the basic syntax of a SPARQL query.   Benchmarks .  A benchmark is a brief phrase describing an action that can\ndemonstrate accomplishment in a given competency.  If competencies are about\nlearning, benchmarks are about doing.  Benchmarks may be used for devising\nhomework assignments, exam questions, or self-testing checklists.  Under the\ncompetency  Understands the basic syntax of a SPARQL query , for example, one\nfinds three benchmarks:   Uses angle brackets for delimiting URIs.  Uses question marks for indicating variables.  Uses PREFIX for base URIs.   The strictly topical structure of the Competency Index entailed the following \ndesign choices:   The CI avoids classifying competencies or benchmarks by level of difficulty\n  because it makes no particular assumptions about the background or skills\n  sets of learners.  Concepts that are comparatively easy for a library science \n  student may be hard for a computer science student, and vice versa.  The CI avoids implying an inherent order to the topics.",
            "title": "Structure of the Competency Index"
        },
        {
            "location": "/structure/#writing-effective-competencies-and-benchmarks",
            "text": "Limit each competency or benchmark to one sentence of circa 140 characters,\n   the length of a Twitter posting.  If a thought seems to require more words,\n   consider splitting it into two simpler thoughts.    Begin each competency or benchmark with an action verb.  Competencies, which\n   are about learning and understanding, start with verbs such as  understands ,\n    knows ,  recognizes , or  differentiates .  Benchmarks, which are about\n   doing, start with verbs such as  uses ,  expresses , or  demonstrates, \n   or with more task-specific verbs such as _distills  or\n    converts .    Spell out acronyms at least once.  For example, the competency that\n   introducing OWL says:  Knows Web Ontology Language, or OWL (2004), an RDF\n   vocabulary of properties and classes that extend support for expressive data\n   modeling and automated inferencing (reasoning).   Use your judgement in this\n   regard; in a competency index about Linked Data, frequently used acronyms\n   such as  OWL  and  URI  need only be spelled out once.    Include historical context if possible.  Linked Data technology has largely\n   evolved over the past quarter century and continues to evolve.  Knowing the\n   year when a technology or concept was introduced helps readers, for example:\n    World Wide Web (1989) ,  HTTP (1991+) ,  URIs (1994) ,  OWL (2004) , and \n    Linked Data (2006) .    Include enough detail to characterize the nature of competency in a domain.\n   Competencies and benchmarks should not aim at covering all features of a\n   technology, in the manner of a reference manual.  Attempts at\n   comprehensiveness risk making the CI brittle in the face of inevitable\n   change, and they risk making the CI boring to read.    Draw attention to ambiguity in the definition or use of terminology.  One \n   competency reads:  Knows that the word \"ontology\" is amiguous, referring to \n   any RDF vocabulary, but more typically a set of OWL classes and properties \n   designed to support inferencing in a specific domain .    Enlarge the set of topics covered by the CI with prudence, bearing in mind\n   adding topic clusters enlarges the scope of the CI as a whole.  The starter\n   set of topics for this CI were originally brainstormed in 2012, at a\n   workshop of experts convened by a previous IMLS-funded project, Learning\n   Linked Data, and there is no expectation that this scope will remain static.\n   For example, there is a recognized need to define competencies related to\n   knowledge organization systems, but this feels like a topic big enough to \n   require its own competency index.",
            "title": "Writing effective competencies and benchmarks"
        },
        {
            "location": "/style/",
            "text": "Style and editorial guidelines\n\n\nStructure of the Competency Index\n\n\nThe Competency Index is structured into \ntopics\n (and higher-level \ntopic\nclusters\n), \ncompetencies\n, and \nbenchmarks\n:\n\n\nTopics\n.  A topic is a theme under which a set of competencies are grouped,\nsuch as \nDesigning RDF-based vocabularies\n and \nMaintaining RDF vocabularies\n.\nTopics are grouped under higher-level topic clusters, such as \nRDF vocabularies\nand application profiles\n.  \n\n\nCompetencies\n.  A competency is a brief phrase characterizing knowledge\n(facts, insights, habits of mind, or skills) that may be learned.\nCompetencies may be used as building blocks for constructing self-learning\nplans, university courses, or even entire curricula.  Under the topic \nQuerying\nRDF Data\n, for example, one finds two competencies:\n\n\n\n\nUnderstands that a SPARQL query matches an RDF graph against a pattern of\n  triples with fixed and variable values.\n\n\nUnderstands the basic syntax of a SPARQL query.\n\n\n\n\nBenchmarks\n.  A benchmark is a brief phrase describing an action that can\ndemonstrate accomplishment in a given competency.  If competencies are about\nlearning, benchmarks are about doing.  Benchmarks may be used for devising\nhomework assignments, exam questions, or self-testing checklists.  Under the\ncompetency \nUnderstands the basic syntax of a SPARQL query\n, for example, one\nfinds three benchmarks:\n\n\n\n\nUses angle brackets for delimiting URIs.\n\n\nUses question marks for indicating variables.\n\n\nUses PREFIX for base URIs.\n\n\n\n\nThe strictly topical structure of the Competency Index entailed the following \ndesign choices:\n\n\n\n\nThe CI avoids classifying competencies or benchmarks by level of difficulty\n  because it makes no particular assumptions about the background or skills\n  sets of learners.  Concepts that are comparatively easy for a library science \n  student may be hard for a computer science student, and vice versa.\n\n\nThe CI avoids implying an inherent order to the topics.\n\n\n\n\nWriting effective competencies and benchmarks\n\n\n\n\n\n\nLimit each competency or benchmark to one sentence of circa 140 characters,\n   the length of a Twitter posting.  If a thought seems to require more words,\n   consider splitting it into two simpler thoughts.\n\n\n\n\n\n\nBegin each competency or benchmark with an action verb.  Competencies, which\n   are about learning and understanding, start with verbs such as \nunderstands\n,\n   \nknows\n, \nrecognizes\n, or \ndifferentiates\n.  Benchmarks, which are about\n   doing, start with verbs such as \nuses\n, \nexpresses\n, or \ndemonstrates, \n   or with more task-specific verbs such as _distills\n or\n   \nconverts\n.\n\n\n\n\n\n\nSpell out acronyms at least once.  For example, the competency that\n   introducing OWL says: \nKnows Web Ontology Language, or OWL (2004), an RDF\n   vocabulary of properties and classes that extend support for expressive data\n   modeling and automated inferencing (reasoning).\n  Use your judgement in this\n   regard; in a competency index about Linked Data, frequently used acronyms\n   such as \nOWL\n and \nURI\n need only be spelled out once.\n\n\n\n\n\n\nInclude historical context if possible.  Linked Data technology has largely\n   evolved over the past quarter century and continues to evolve.  Knowing the\n   year when a technology or concept was introduced helps readers, for example:\n   \nWorld Wide Web (1989)\n, \nHTTP (1991+)\n, \nURIs (1994)\n, \nOWL (2004)\n, and \n   \nLinked Data (2006)\n.\n\n\n\n\n\n\nInclude enough detail to characterize the nature of competency in a domain.\n   Competencies and benchmarks should not aim at covering all features of a\n   technology, in the manner of a reference manual.  Attempts at\n   comprehensiveness risk making the CI brittle in the face of inevitable\n   change, and they risk making the CI boring to read.\n\n\n\n\n\n\nDraw attention to ambiguity in the definition or use of terminology.  One \n   competency reads: \nKnows that the word \"ontology\" is amiguous, referring to \n   any RDF vocabulary, but more typically a set of OWL classes and properties \n   designed to support inferencing in a specific domain\n.\n\n\n\n\n\n\nEnlarge the set of topics covered by the CI with prudence, bearing in mind\n   adding topic clusters enlarges the scope of the CI as a whole.  The starter\n   set of topics for this CI were originally brainstormed in 2012, at a\n   workshop of experts convened by a previous IMLS-funded project, Learning\n   Linked Data, and there is no expectation that this scope will remain static.\n   For example, there is a recognized need to define competencies related to\n   knowledge organization systems, but this feels like a topic big enough to \n   require its own competency index.",
            "title": "Style"
        },
        {
            "location": "/style/#style-and-editorial-guidelines",
            "text": "",
            "title": "Style and editorial guidelines"
        },
        {
            "location": "/style/#structure-of-the-competency-index",
            "text": "The Competency Index is structured into  topics  (and higher-level  topic\nclusters ),  competencies , and  benchmarks :  Topics .  A topic is a theme under which a set of competencies are grouped,\nsuch as  Designing RDF-based vocabularies  and  Maintaining RDF vocabularies .\nTopics are grouped under higher-level topic clusters, such as  RDF vocabularies\nand application profiles .    Competencies .  A competency is a brief phrase characterizing knowledge\n(facts, insights, habits of mind, or skills) that may be learned.\nCompetencies may be used as building blocks for constructing self-learning\nplans, university courses, or even entire curricula.  Under the topic  Querying\nRDF Data , for example, one finds two competencies:   Understands that a SPARQL query matches an RDF graph against a pattern of\n  triples with fixed and variable values.  Understands the basic syntax of a SPARQL query.   Benchmarks .  A benchmark is a brief phrase describing an action that can\ndemonstrate accomplishment in a given competency.  If competencies are about\nlearning, benchmarks are about doing.  Benchmarks may be used for devising\nhomework assignments, exam questions, or self-testing checklists.  Under the\ncompetency  Understands the basic syntax of a SPARQL query , for example, one\nfinds three benchmarks:   Uses angle brackets for delimiting URIs.  Uses question marks for indicating variables.  Uses PREFIX for base URIs.   The strictly topical structure of the Competency Index entailed the following \ndesign choices:   The CI avoids classifying competencies or benchmarks by level of difficulty\n  because it makes no particular assumptions about the background or skills\n  sets of learners.  Concepts that are comparatively easy for a library science \n  student may be hard for a computer science student, and vice versa.  The CI avoids implying an inherent order to the topics.",
            "title": "Structure of the Competency Index"
        },
        {
            "location": "/style/#writing-effective-competencies-and-benchmarks",
            "text": "Limit each competency or benchmark to one sentence of circa 140 characters,\n   the length of a Twitter posting.  If a thought seems to require more words,\n   consider splitting it into two simpler thoughts.    Begin each competency or benchmark with an action verb.  Competencies, which\n   are about learning and understanding, start with verbs such as  understands ,\n    knows ,  recognizes , or  differentiates .  Benchmarks, which are about\n   doing, start with verbs such as  uses ,  expresses , or  demonstrates, \n   or with more task-specific verbs such as _distills  or\n    converts .    Spell out acronyms at least once.  For example, the competency that\n   introducing OWL says:  Knows Web Ontology Language, or OWL (2004), an RDF\n   vocabulary of properties and classes that extend support for expressive data\n   modeling and automated inferencing (reasoning).   Use your judgement in this\n   regard; in a competency index about Linked Data, frequently used acronyms\n   such as  OWL  and  URI  need only be spelled out once.    Include historical context if possible.  Linked Data technology has largely\n   evolved over the past quarter century and continues to evolve.  Knowing the\n   year when a technology or concept was introduced helps readers, for example:\n    World Wide Web (1989) ,  HTTP (1991+) ,  URIs (1994) ,  OWL (2004) , and \n    Linked Data (2006) .    Include enough detail to characterize the nature of competency in a domain.\n   Competencies and benchmarks should not aim at covering all features of a\n   technology, in the manner of a reference manual.  Attempts at\n   comprehensiveness risk making the CI brittle in the face of inevitable\n   change, and they risk making the CI boring to read.    Draw attention to ambiguity in the definition or use of terminology.  One \n   competency reads:  Knows that the word \"ontology\" is amiguous, referring to \n   any RDF vocabulary, but more typically a set of OWL classes and properties \n   designed to support inferencing in a specific domain .    Enlarge the set of topics covered by the CI with prudence, bearing in mind\n   adding topic clusters enlarges the scope of the CI as a whole.  The starter\n   set of topics for this CI were originally brainstormed in 2012, at a\n   workshop of experts convened by a previous IMLS-funded project, Learning\n   Linked Data, and there is no expectation that this scope will remain static.\n   For example, there is a recognized need to define competencies related to\n   knowledge organization systems, but this feels like a topic big enough to \n   require its own competency index.",
            "title": "Writing effective competencies and benchmarks"
        },
        {
            "location": "/process/",
            "text": "methodology).  Stylistic consistency.\n\n\n\n\nDescribe Editorial Board process: constitution, members, process (members, meetings, decision method)\n\n\nPick one pull request, link to its diff\n\n\nWrite to CIEB: status of pull requests. Proposal Dec telecon. Still using \"manual\" update. Propose style cleanup.",
            "title": "Process"
        },
        {
            "location": "/test/",
            "text": "[A]\n Interacting with RDF data\n\n\n\n\n[B]\n Finding RDF data\n\n\n[C]\n Knows relevant resources for discovering existing Linked Data datasets.\n\n\n[C]\n Retrieves and accesses RDF data from the \"open Web\".\n\n\n[C]\n Monitors and updates lists which report the status of SPARQL endpoints.\n\n\n[C]\n Uses available vocabularies for dataset description to support their discovery.\n\n\n[C]\n Registers datasets with relevant services for discovery.\n\n\n\n\n\n\n[B]\n Processing RDF data using programming languages.\n\n\n[C]\n Understands how components of the RDF data model (datasets, graphs, statements, and various types of node) are expressed in the RDF library of a given programming language by constructs such as object-oriented classes.\n\n\n[D]\n Uses an RDF programming library to serialize RDF data in available syntaxes.",
            "title": "Test"
        },
        {
            "location": "/test/#a-interacting-with-rdf-data",
            "text": "[B]  Finding RDF data  [C]  Knows relevant resources for discovering existing Linked Data datasets.  [C]  Retrieves and accesses RDF data from the \"open Web\".  [C]  Monitors and updates lists which report the status of SPARQL endpoints.  [C]  Uses available vocabularies for dataset description to support their discovery.  [C]  Registers datasets with relevant services for discovery.    [B]  Processing RDF data using programming languages.  [C]  Understands how components of the RDF data model (datasets, graphs, statements, and various types of node) are expressed in the RDF library of a given programming language by constructs such as object-oriented classes.  [D]  Uses an RDF programming library to serialize RDF data in available syntaxes.",
            "title": "[A] Interacting with RDF data"
        }
    ]
}